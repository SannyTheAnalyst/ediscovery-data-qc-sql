\# eDiscovery Data Processing \& Quality Control (SQL Server)

\### Load File Validation, Multi-Stream Ingestion, and QC Verification Aligned to LAW / Relativity Workflows



\## Overview

This project simulates an eDiscovery Data Specialist workflow focused on processing, validating, and quality-controlling large volumes of document metadata prior to production. The workflow mirrors core operations performed in tools such as LAW and Relativity, with an emphasis on accuracy, auditability, and repeatable QC logic.



\## Key Capabilities Demonstrated

\- Load file modification and standardization

\- Multi-stream data ingestion into staging tables

\- Record count reconciliation across data sources

\- Required-field validation and exception detection

\- File type compliance checks

\- Hash-based duplicate detection

\- QC documentation for audit and review



\## Repository Structure

\- `docs/` — QC summary and documentation

\- `data/` — Sample raw and production export files

\- `sql/` — SQL scripts for loading, validation, and QC checks



\## Tools Used

\- SQL Server / Azure Data Studio

\- CSV-based load file simulation

\- Git \& GitHub for version control



\## Notes

All data used in this repository is simulated and contains no sensitive or client information.



